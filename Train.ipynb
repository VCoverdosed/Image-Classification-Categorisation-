{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed91fa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skimage.feature import hog\n",
    "from skimage.io import imread\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a7b6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_keypoints(image, num_keypoints):\n",
    "    keypoints = []\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    # 计算每个网格的大小\n",
    "    grid_size_x = width // int(np.sqrt(num_keypoints))\n",
    "    grid_size_y = height // int(np.sqrt(num_keypoints))\n",
    "    \n",
    "    # 在每个网格中选择一个点作为关键点\n",
    "    for i in range(0, width, grid_size_x):\n",
    "        for j in range(0, height, grid_size_y):\n",
    "            x = i + grid_size_x // 2\n",
    "            y = j + grid_size_y // 2\n",
    "            keypoints.append(cv2.KeyPoint(x, y, 10))  # 10是关键点的尺度\n",
    "            \n",
    "            # 如果已经选择了足够多的关键点，则停止\n",
    "            if len(keypoints) == num_keypoints:\n",
    "                return keypoints\n",
    "    \n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8763f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image_path, num_keypoints):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints = random_keypoints(img, num_keypoints)\n",
    "    keypoints, descriptors = sift.compute(img, keypoints)\n",
    "    return descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1da875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(train_data_dir, num_clusters, num_keypoints):\n",
    "    # 定义提取特征的函数\n",
    "    def extract_features_from_folder(folder_path, num_keypoints=num_keypoints):\n",
    "        features = []\n",
    "        for img_name in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, img_name)\n",
    "            img_features = extract_features(img_path,num_keypoints=num_keypoints)\n",
    "            features.extend(img_features)\n",
    "        return features\n",
    "    \n",
    "    # 遍历每个类别文件夹并提取特征\n",
    "    all_features = []\n",
    "    for class_name in os.listdir(train_data_dir):\n",
    "        class_dir = os.path.join(train_data_dir, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "        class_features = extract_features_from_folder(class_dir, num_keypoints=num_keypoints)\n",
    "        all_features.extend(class_features)\n",
    "    \n",
    "    # 将提取的特征转换成特征矩阵\n",
    "    features_matrix = np.array(all_features)\n",
    "    \n",
    "    # 应用K均值算法进行聚类\n",
    "    kmeans = KMeans(n_clusters=num_clusters, n_init='auto')\n",
    "    kmeans.fit(features_matrix)\n",
    "    \n",
    "    # 得到词典，即聚类中心\n",
    "    vocabulary = kmeans.cluster_centers_\n",
    "    \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5089f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_files_and_integer_labels(base_dir, image_extensions=['.jpg', '.jpeg', '.png', '.gif', '.bmp']):\n",
    "    # Sort folder names case-insensitively and create the mapping\n",
    "    folders = sorted([d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))], key=lambda x: x.lower())\n",
    "    folder_to_int_label = {folder.lower(): i+1 for i, folder in enumerate(folders)}  # Mapping is lowercase\n",
    "\n",
    "    image_files = []\n",
    "    int_labels = []\n",
    "\n",
    "    for folder in folders:\n",
    "        folder_path = os.path.join(base_dir, folder)\n",
    "        for extension in image_extensions:\n",
    "            for file in glob.glob(os.path.join(folder_path, '*' + extension)):\n",
    "                image_files.append(file)\n",
    "                # Assign labels using lowercase folder name for consistency\n",
    "                int_labels.append(folder_to_int_label[folder.lower()])\n",
    "    \n",
    "    return image_files, int_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e3b41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hist(image_files, int_labels, vocabulary,num_keypoints):\n",
    "    histograms = []\n",
    "    for image_file, label in zip(image_files, int_labels):\n",
    "        img_features = extract_features(image_file,num_keypoints=num_keypoints)  # 提取图像特征\n",
    "        hist = np.zeros(len(vocabulary))\n",
    "        for feature in img_features:\n",
    "            # 计算图像的直方图\n",
    "            distances = np.linalg.norm(vocabulary - feature, axis=1)\n",
    "            nearest_word_idx = np.argmin(distances)\n",
    "            hist[nearest_word_idx] += 1\n",
    "        histograms.append((hist, label))  # 存储直方图和对应的标签\n",
    "    return histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699beea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = \"/users/huangsicheng/Desktop/DSA5203/Assignment3/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fd1451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data_dir, num_clusters, num_keypoints):\n",
    "    # 步骤1：构建词汇表\n",
    "    vocabulary = build_vocabulary(train_data_dir, num_clusters=num_clusters,num_keypoints=num_keypoints)\n",
    "    # 步骤2:获取图片文件路径并按字母顺序标签\n",
    "    image_files, int_labels = get_image_files_and_integer_labels(train_data_dir)\n",
    "    # 步骤3：获取视觉单词的直方图\n",
    "    histograms = get_hist(image_files, int_labels, vocabulary,num_keypoints)\n",
    "\n",
    "    # 获取每个类别的总样本数\n",
    "    num_samples_per_class = len(image_files) // len(set(int_labels))\n",
    "\n",
    "    # 创建字典以保存每个类别的图片索引\n",
    "    class_indices = defaultdict(list)\n",
    "    for idx, (_, label) in enumerate(histograms):\n",
    "        class_indices[label].append(idx)\n",
    "\n",
    "    # 计算应该分配给测试集的每个类别的图片数量\n",
    "    num_test_samples_per_class = {label: num_samples_per_class // 20 for label in set(int_labels)}\n",
    "\n",
    "    # 初始化训练集和测试集\n",
    "    X_train, X_test, y_train, y_test = [], [], [], []\n",
    "\n",
    "    # 从每个类别中分别选择图片添加到训练集和测试集中\n",
    "    for label, indices in class_indices.items():\n",
    "        # 按比例选择图片索引\n",
    "        np.random.seed(42)\n",
    "        test_indices = np.random.choice(indices, num_test_samples_per_class[label], replace=False)\n",
    "        train_indices = [idx for idx in indices if idx not in test_indices]\n",
    "\n",
    "        # 添加到训练集和测试集\n",
    "        for idx in train_indices:\n",
    "            X_train.append(histograms[idx][0])\n",
    "            y_train.append(histograms[idx][1])\n",
    "        for idx in test_indices:\n",
    "            X_test.append(histograms[idx][0])\n",
    "            y_test.append(histograms[idx][1])\n",
    "\n",
    "    # 转换为NumPy数组\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "    X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "\n",
    "    # 初始化高斯核支持向量机分类器\n",
    "    svm_classifier = SVC(kernel='rbf')\n",
    "\n",
    "    # 使用一对所有方法包装分类器\n",
    "    ovr_classifier = OneVsRestClassifier(svm_classifier)\n",
    "\n",
    "    # 在训练集上训练分类器\n",
    "    ovr_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # 在测试集上进行预测\n",
    "    y_pred = ovr_classifier.predict(X_test)\n",
    "\n",
    "    # 计算准确率\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fae477",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = train(train_data_dir, num_clusters=400, num_keypoints=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552f6807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
